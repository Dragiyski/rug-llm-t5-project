{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ed409f",
   "metadata": {},
   "source": [
    "Dependencies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16680259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "import transformers, datasets, pickle, multiprocessing, peft, evaluate, py7zr, functools, accelerate\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a200ea1",
   "metadata": {},
   "source": [
    "Global GPU..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_device = t.device('cpu')\n",
    "model_run_device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d48cb",
   "metadata": {},
   "source": [
    "Tokenizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d28bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26088082",
   "metadata": {},
   "source": [
    "Functions for predicting and decoding. Split into parts due to memory limitations. Adjust parts as necessary. It shouldn't really matter how many you have as long as it isn't excessive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48457de",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = 8\n",
    "\n",
    "def prediction_by_parts(trainer, data, file_name):\n",
    "    ni = int(800/parts)\n",
    "    for n in range(parts):\n",
    "        pred = trainer.model.generate(t.tensor(data[\"test\"][\"input_ids\"][(ni * n):(ni*(n+1))]).to(model_run_device), \n",
    "                                      max_length=64, min_length=4, \n",
    "                                      length_penalty=1.2, num_beams=4, \n",
    "                                      early_stopping=True, \n",
    "                                      repetition_penalty=3.0, no_repeat_ngram_size=3)\n",
    "        with open(f'./predictions/{file_name}_predictions_{n}.pickle', 'wb') as file:\n",
    "            pickle.dump(pred, file)\n",
    "\n",
    "def prediction_decoding_by_parts(file_name):\n",
    "    for n in range(parts):\n",
    "        with open(f'./predictions/{file_name}_predictions_{n}.pickle', 'rb') as file:\n",
    "            pred = pickle.load(file)\n",
    "        pred_summaries = tokenizer.batch_decode(pred, skip_special_tokens=True)\n",
    "        with open(f'./readable-predictions/readable_{file_name}_predictions_{n}.pickle', 'wb') as file:\n",
    "            pickle.dump(pred_summaries, file)\n",
    "            \n",
    "def combine_predictions(file_name):\n",
    "    all_predictions = []\n",
    "    for n in range(parts):\n",
    "        with open(f'./readable-predictions/readable_{file_name}_predictions_{n}.pickle', 'rb') as file:\n",
    "            part_predictions = pickle.load(file)\n",
    "            all_predictions.extend(part_predictions)\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc2efc",
   "metadata": {},
   "source": [
    "Open the full token sets from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e80214",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./preprocessing/cnn_tokens.pickle', 'rb') as file:\n",
    "    cnn_tokens = pickle.load(file)\n",
    "with open(f'./preprocessing/samsum_tokens.pickle', 'rb') as file:\n",
    "    samsum_tokens = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e19b1",
   "metadata": {},
   "source": [
    "Predict and save predictions for each trainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08838fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./models/base_trainer.pickle', 'rb') as file:\n",
    "    base_trainer = pickle.load(file)\n",
    "base_trainer.model.to(model_run_device)\n",
    "prediction_by_parts(base_trainer, samsum_tokens, \"base_none_to_samsum\")\n",
    "prediction_by_parts(base_trainer, cnn_tokens, \"base_none_to_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./models/cnn_trainer.pickle', 'rb') as file:\n",
    "    cnn_trainer = pickle.load(file)\n",
    "cnn_trainer.model.to(model_run_device)\n",
    "prediction_by_parts(cnn_trainer, samsum_tokens, \"lora_cnn_to_samsum\")\n",
    "prediction_by_parts(cnn_trainer, cnn_tokens, \"lora_cnn_to_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3830705",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./models/samsum_trainer.pickle', 'rb') as file:\n",
    "    samsum_trainer = pickle.load(file)\n",
    "samsum_trainer.model.to(model_run_device)\n",
    "prediction_by_parts(samsum_trainer, samsum_tokens, \"lora_samsum_to_samsum\")\n",
    "prediction_by_parts(samsum_trainer, cnn_tokens, \"lora_samsum_to_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./models/mixed_trainer.pickle', 'rb') as file:\n",
    "    mixed_trainer = pickle.load(file)\n",
    "mixed_trainer.model.to(model_run_device)\n",
    "prediction_by_parts(mixed_trainer, samsum_tokens, \"lora_mixed_to_samsum\")\n",
    "prediction_by_parts(mixed_trainer, cnn_tokens, \"lora_mixed_to_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./models/cnn_FFT_trainer.pickle', 'rb') as file:\n",
    "    cnn_FFT_trainer = pickle.load(file)\n",
    "cnn_FFT_trainer.model.to(model_run_device)\n",
    "prediction_by_parts(cnn_FFT_trainer, samsum_tokens, \"FFT_cnn_to_samsum\")\n",
    "prediction_by_parts(cnn_FFT_trainer, cnn_tokens, \"FFT_cnn_to_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./models/samsum_FFT_trainer.pickle', 'rb') as file:\n",
    "    samsum_FFT_trainer = pickle.load(file)\n",
    "samsum_FFT_trainer.model.to(model_run_device)\n",
    "prediction_by_parts(samsum_FFT_trainer, samsum_tokens, \"FFT_samsum_to_samsum\")\n",
    "prediction_by_parts(samsum_FFT_trainer, cnn_tokens, \"FFT_samsum_to_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa990f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./models/mixed_FFT_trainer.pickle', 'rb') as file:\n",
    "    mixed_FFT_trainer = pickle.load(file)\n",
    "mixed_FFT_trainer.model.to(model_run_device)\n",
    "prediction_by_parts(mixed_FFT_trainer, samsum_tokens, \"FFT_mixed_to_samsum\")\n",
    "prediction_by_parts(mixed_FFT_trainer, cnn_tokens, \"FFT_mixed_to_cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98cc7d",
   "metadata": {},
   "source": [
    "Define a list of names which can be used to access predictions in the dictionaries they're be stored in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsum_names = [\n",
    "    \"base_none_to_samsum\",\n",
    "    \"FFT_cnn_to_samsum\",\n",
    "    \"FFT_samsum_to_samsum\",\n",
    "    \"FFT_mixed_to_samsum\",\n",
    "    \"lora_cnn_to_samsum\",\n",
    "    \"lora_samsum_to_samsum\",\n",
    "    \"lora_mixed_to_samsum\"\n",
    "]\n",
    "\n",
    "cnn_names = [\n",
    "    \"base_none_to_cnn\",\n",
    "    \"FFT_cnn_to_cnn\",\n",
    "    \"FFT_samsum_to_cnn\",\n",
    "    \"FFT_mixed_to_cnn\",\n",
    "    \"lora_cnn_to_cnn\",\n",
    "    \"lora_samsum_to_cnn\",\n",
    "    \"lora_mixed_to_cnn\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e2729",
   "metadata": {},
   "source": [
    "Decode the tokenized predictions into a readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8104946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in samsum_names: prediction_decoding_by_parts(name) \n",
    "for name in cnn_names: prediction_decoding_by_parts(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f95c5",
   "metadata": {},
   "source": [
    "Reassemble the parts. Doubles as loading in case you already did all the steps before and have parts saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93648c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsum_preds = {\n",
    "    name: combine_predictions(name) for name in samsum_names\n",
    "}\n",
    "cnn_preds = {\n",
    "    name: combine_predictions(name) for name in cnn_names\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d235b",
   "metadata": {},
   "source": [
    "Load natural language testing datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d10041",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./preprocessing/cnn_test.pickle', 'rb') as file:\n",
    "    cnn_test = pickle.load(file)\n",
    "with open(f'./preprocessing/samsum_test.pickle', 'rb') as file:\n",
    "    samsum_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4aa0c8",
   "metadata": {},
   "source": [
    "Compute rouge score for each trainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_rouge_scores(predictions, references):\n",
    "    return rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "rouge_scores_samsum = {\n",
    "    name: compute_rouge_scores(predictions, samsum_test[\"highlights\"]) for name, predictions in samsum_preds.items()\n",
    "}\n",
    "\n",
    "rouge_scores_cnn = {\n",
    "    name: compute_rouge_scores(predictions, cnn_test[\"highlights\"]) for name, predictions in cnn_preds.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f85482",
   "metadata": {},
   "source": [
    "Compute bert score for each trainer. Also compute average for table reasons. Raws used for anova later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d686ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def calc_bert_average(bert_scores):\n",
    "    bert_averages = {\n",
    "        \"precision\": np.mean(bert_scores[\"precision\"]),\n",
    "        \"recall\": np.mean(bert_scores[\"recall\"]),\n",
    "        \"f1\": np.mean(bert_scores[\"f1\"])\n",
    "    }\n",
    "    return bert_averages\n",
    "\n",
    "bert_scores_samsum = {\n",
    "    name: bert_metric.compute(predictions=predictions, references=samsum_test[\"highlights\"], lang=\"en\", model_type=\"t5-base\") for name, predictions in samsum_preds.items()\n",
    "}\n",
    "bert_scores_cnn= {\n",
    "    name: bert_metric.compute(predictions=predictions, references=cnn_test[\"highlights\"], lang=\"en\", model_type=\"t5-base\") for name, predictions in cnn_preds.items()\n",
    "}\n",
    "\n",
    "bert_scores_average_samsum = {\n",
    "    name: calc_bert_average(scores) for name, scores in bert_scores_samsum.items()\n",
    "}\n",
    "bert_scores_average_cnn = {\n",
    "    name: calc_bert_average(scores) for name, scores in bert_scores_cnn.items()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1381fd",
   "metadata": {},
   "source": [
    "Function for printing dictionaries prettily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(scores_dict):\n",
    "    for model_name, metrics in scores_dict.items():\n",
    "        print(f\"Model: {model_name}\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "        print() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3d727",
   "metadata": {},
   "source": [
    "Print the metric averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67e618",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"== Rouge Scores CNN ==\")\n",
    "print_scores(rouge_scores_cnn)\n",
    "print(\"== Rouge Scores SAMSum ==\")\n",
    "print_scores(rouge_scores_samsum)\n",
    "print(\"--------------------------\\n\")\n",
    "print(\"== BERT Scores CNN ==\")\n",
    "print_scores(bert_scores_average_cnn)\n",
    "print(\"== BERT Scores SAMSum ==\")\n",
    "print_scores(bert_scores_average_samsum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5cac38",
   "metadata": {},
   "source": [
    "Set up the dataframe of Bert Precision in long data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50530672",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = { \n",
    "    'test_data': ['cnn']*(7*800) + ['samsum']*(7*800),\n",
    "    'model_type': ['base']*800 + ['FFT']*(3*800) + ['lora']*(3*800) + ['base']*800 + ['FFT']*(3*800) + ['lora']*(3*800),\n",
    "    'train_data': ['none']*800 + [\"cnn\"]*800 + [\"samsum\"] * 800 + [\"mixed\"] * 800 + [\"cnn\"]*800 + [\"samsum\"] * 800 + [\"mixed\"] * 800 + \n",
    "        ['none']*800 + [\"cnn\"]*800 + [\"samsum\"] * 800 + [\"mixed\"] * 800 + [\"cnn\"]*800 + [\"samsum\"] * 800 + [\"mixed\"] * 800,\n",
    "    'precision': [precision for name in cnn_names for precision in bert_scores_cnn[name][\"precision\"]] + \n",
    "        [precision for name in samsum_names for precision in bert_scores_samsum[name][\"precision\"]]\n",
    "}\n",
    "bert_df = pd.DataFrame(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf95003",
   "metadata": {},
   "outputs": [],
   "source": [
    "Perform three-way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols(\"\"\"precision ~ C(model_type) + C(train_data) + C(test_data) +\n",
    "               C(model_type):C(train_data) + C(model_type):C(test_data) + C(train_data):C(test_data) +\n",
    "               C(model_type):C(train_data):C(test_data)\"\"\", data=bert_df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da8289",
   "metadata": {},
   "source": [
    "Function for printing out test texts as well as corresponding label and predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07725619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualitative_analysis(index, data_set):\n",
    "    if data_set == \"cnn\":\n",
    "        print(\"Input: \", cnn_test[\"article\"][index], \"\\n\")\n",
    "        print(\"Label: \", cnn_test[\"highlights\"][index], \"\\n\")\n",
    "        print(\"Base Prediction: \", cnn_preds[\"base_none_to_cnn\"][index], \"\\n\")\n",
    "        print(\"FFT CNN Prediction: \", cnn_preds[\"FFT_cnn_to_cnn\"][index], \"\\n\")\n",
    "        print(\"FFT Samsum Prediction: \", cnn_preds[\"FFT_samsum_to_cnn\"][index], \"\\n\")\n",
    "        print(\"FFT Mixed Prediction: \", cnn_preds[\"FFT_mixed_to_cnn\"][index])\n",
    "        print(\"LoRA CNN Prediction: \", cnn_preds[\"lora_cnn_to_cnn\"][index], \"\\n\")\n",
    "        print(\"LoRA Samsum Prediction: \", cnn_preds[\"lora_samsum_to_cnn\"][index], \"\\n\")\n",
    "        print(\"LoRA Mixed Prediction: \", cnn_preds[\"lora_mixed_to_cnn\"][index])\n",
    "    else:\n",
    "        print(\"Input: \", samsum_test[\"article\"][index], \"\\n\")\n",
    "        print(\"Label: \", samsum_test[\"highlights\"][index], \"\\n\")\n",
    "        print(\"Base Prediction: \", samsum_preds[\"base_none_to_samsum\"][index], \"\\n\")\n",
    "        print(\"FFT CNN Prediction: \", samsum_preds[\"FFT_cnn_to_samsum\"][index], \"\\n\")\n",
    "        print(\"FFT Samsum Prediction: \", samsum_preds[\"FFT_samsum_to_samsum\"][index], \"\\n\")\n",
    "        print(\"FFT Mixed Prediction: \", samsum_preds[\"FFT_mixed_to_samsum\"][index])\n",
    "        print(\"LoRA CNN Prediction: \", samsum_preds[\"lora_cnn_to_samsum\"][index], \"\\n\")\n",
    "        print(\"LoRA Samsum Prediction: \", samsum_preds[\"lora_samsum_to_samsum\"][index], \"\\n\")\n",
    "        print(\"LoRA Mixed Prediction: \", samsum_preds[\"lora_mixed_to_samsum\"][index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fae81",
   "metadata": {},
   "source": [
    "Call of the above function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f58e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "qualitative_analysis(index, \"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "qualitative_analysis(index, \"cnn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (GPU)",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
